{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408d1479",
   "metadata": {},
   "source": [
    "# üöÄ EXECUTION INSTRUCTIONS\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Run cells in this exact order to avoid errors:**\n",
    "\n",
    "1. **Import Libraries** (Cell 2) - Run first\n",
    "2. **Helper Functions** (Cell 3) - Run second  \n",
    "3. **Load Dataset** (Cell 5) - Run third\n",
    "4. **Preprocess Text** (Cell 7) - Run fourth\n",
    "5. **Feature Extraction** (Cell 9) - Run fifth\n",
    "6. **Choose your training approach:**\n",
    "   - **Enhanced Logistic Regression** (Cell 10) - **RECOMMENDED** for Streamlit compatibility\n",
    "   - **Standard Logistic Regression** (Cell 11) - Alternative approach\n",
    "   - **BiLSTM Model** (Cell 13) - Deep learning approach\n",
    "   - **BERT Model** (Cell 16) - Transformer approach\n",
    "\n",
    "**‚úÖ For Streamlit app compatibility, make sure to run Cell 10 (Enhanced Logistic Regression)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9efbee",
   "metadata": {},
   "source": [
    "# AI Text Detector\n",
    "\n",
    "This notebook demonstrates how to detect whether text is AI-generated or human-written using various machine learning and deep learning models. We will:\n",
    "- Load and preprocess a labeled dataset\n",
    "- Extract features (TF-IDF, embeddings, BERT)\n",
    "- Train and evaluate Logistic Regression, SVM, BiLSTM, and BERT models\n",
    "- Compare their performance using accuracy, precision, recall, and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392f0af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AIDetector\\AITextDetector\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mohgu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mohgu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mohgu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mohgu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mohgu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mohgu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3006da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def show_metrics(name, y_true, y_pred):\n",
    "    \"\"\"Display evaluation metrics for a model\"\"\"\n",
    "    print(f'--- {name} ---')\n",
    "    print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "    print('Precision:', precision_score(y_true, y_pred))\n",
    "    print('Recall:', recall_score(y_true, y_pred))\n",
    "    print('F1 Score:', f1_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "def simple_preprocess(text):\n",
    "    \"\"\"Simple text preprocessing for quick training\"\"\"\n",
    "    return str(text).lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62a775",
   "metadata": {},
   "source": [
    "## Load Labeled Dataset\n",
    "\n",
    "Upload your CSV file with columns `text` and `label` (0 = Human, 1 = AI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d4e55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permission denied for: AI_Human.csv\n",
      "Successfully loaded dataset from: AI_Human.csv/AI_Human.csv\n",
      "Dataset shape: (487235, 2)\n",
      "Successfully loaded dataset from: AI_Human.csv/AI_Human.csv\n",
      "Dataset shape: (487235, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import os\n",
    "\n",
    "# Try different paths for the CSV file\n",
    "csv_paths = [\n",
    "    'AI_Human.csv',\n",
    "    'AI_Human.csv/AI_Human.csv',  # In case it's in a subfolder\n",
    "    os.path.join(os.getcwd(), 'AI_Human.csv')\n",
    "]\n",
    "\n",
    "df = None\n",
    "for path in csv_paths:\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"Successfully loaded dataset from: {path}\")\n",
    "            break\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied for: {path}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "        continue\n",
    "\n",
    "if df is None:\n",
    "    print(\"Could not load dataset. Please check:\")\n",
    "    print(\"1. Close Excel or any program that has the CSV file open\")\n",
    "    print(\"2. Make sure the file exists in the current directory\")\n",
    "    print(\"3. Check file permissions\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"Files in directory: {os.listdir('.')}\")\n",
    "else:\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58d60b",
   "metadata": {},
   "source": [
    "## Preprocess Text\n",
    "\n",
    "Clean, tokenize, lemmatize, and remove stopwords from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a377804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mohgu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mohgu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Cars. Cars have been around since they became ...   \n",
      "1  Transportation is a large necessity in most co...   \n",
      "2  \"America's love affair with it's vehicles seem...   \n",
      "3  How often do you ride in a car? Do you drive a...   \n",
      "4  Cars are a wonderful thing. They are perhaps o...   \n",
      "\n",
      "                                          text_clean  generated  \n",
      "0  car car around since became famous henry ford ...        0.0  \n",
      "1  transportation large necessity country worldwi...        0.0  \n",
      "2  america love affair vehicle seems cooling say ...        0.0  \n",
      "3  often ride car drive one motor vehicle work st...        0.0  \n",
      "4  car wonderful thing perhaps one world greatest...        0.0  \n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK 'punkt' and 'punkt_tab' resources are downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Preprocessing functions\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Check if 'generated' column exists before proceeding\n",
    "if 'generated' in df.columns:\n",
    "    df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "    print(df[['text', 'text_clean', 'generated']].head())\n",
    "else:\n",
    "    print(\"Error: 'generated' column is missing from the dataset. Please check the dataset structure.\")\n",
    "    print(\"Available columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e963bcb",
   "metadata": {},
   "source": [
    "## Feature Extraction and Train-Test Split\n",
    "\n",
    "We will extract features using TF-IDF and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f6bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature shape: (389788, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Ensure required columns exist\n",
    "if 'text_clean' in df.columns and 'generated' in df.columns:\n",
    "    # Extract features using TF-IDF and split the data\n",
    "    X = df['text_clean']\n",
    "    y = df['generated']\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    print('TF-IDF feature shape:', X_train_tfidf.shape)\n",
    "else:\n",
    "    print(\"Error: Required columns 'text_clean' and/or 'generated' are missing from the DataFrame.\")\n",
    "    print(\"Available columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7eb67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for enhanced logistic regression...\n",
      "Creating enhanced TF-IDF vectorizer...\n",
      "Creating enhanced TF-IDF vectorizer...\n",
      "Training enhanced logistic regression...\n",
      "Training enhanced logistic regression...\n",
      "--- Enhanced Logistic Regression (TF-IDF) ---\n",
      "Accuracy: 0.993268135499297\n",
      "Precision: 0.9951977793199167\n",
      "Recall: 0.9867070317875327\n",
      "F1 Score: 0.9909342177998894\n",
      "\n",
      "--- Enhanced Logistic Regression (TF-IDF) ---\n",
      "Accuracy: 0.993268135499297\n",
      "Precision: 0.9951977793199167\n",
      "Recall: 0.9867070317875327\n",
      "F1 Score: 0.9909342177998894\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m show_metrics(\u001b[33m'\u001b[39m\u001b[33mEnhanced Logistic Regression (TF-IDF)\u001b[39m\u001b[33m'\u001b[39m, y_test_simple, y_pred_enhanced)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Save models (these will be compatible with Streamlit)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mjoblib\u001b[49m.dump(tfidf_vectorizer_enhanced, \u001b[33m\"\u001b[39m\u001b[33mtfidf_vectorizer.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m joblib.dump(lr_enhanced, \u001b[33m\"\u001b[39m\u001b[33mlogistic_regression_model.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEnhanced TF-IDF vectorizer saved as \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtfidf_vectorizer.pkl\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "# Enhanced Logistic Regression Training (Alternative Method)\n",
    "# This cell provides an alternative, comprehensive approach to training logistic regression\n",
    "\n",
    "import joblib  # Add missing import for model saving\n",
    "\n",
    "# Prepare data using simple preprocessing\n",
    "print(\"Preparing data for enhanced logistic regression...\")\n",
    "df_copy = df.copy()  # Work with a copy to avoid conflicts\n",
    "df_copy['text_simple'] = df_copy['text'].apply(simple_preprocess)\n",
    "\n",
    "# Split data\n",
    "X_simple = df_copy['text_simple']\n",
    "y_simple = df_copy['generated']\n",
    "X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(\n",
    "    X_simple, y_simple, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TF-IDF vectorizer (compatible with Streamlit app)\n",
    "print(\"Creating enhanced TF-IDF vectorizer...\")\n",
    "tfidf_vectorizer_enhanced = TfidfVectorizer(\n",
    "    max_features=5000, \n",
    "    ngram_range=(1, 2), \n",
    "    stop_words='english'\n",
    ")\n",
    "X_train_tfidf_enhanced = tfidf_vectorizer_enhanced.fit_transform(X_train_simple)\n",
    "X_test_tfidf_enhanced = tfidf_vectorizer_enhanced.transform(X_test_simple)\n",
    "\n",
    "# Train Enhanced Logistic Regression\n",
    "print(\"Training enhanced logistic regression...\")\n",
    "lr_enhanced = LogisticRegression(max_iter=200, random_state=42)\n",
    "lr_enhanced.fit(X_train_tfidf_enhanced, y_train_simple)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_enhanced = lr_enhanced.predict(X_test_tfidf_enhanced)\n",
    "show_metrics('Enhanced Logistic Regression (TF-IDF)', y_test_simple, y_pred_enhanced)\n",
    "\n",
    "# Save models (these will be compatible with Streamlit)\n",
    "joblib.dump(tfidf_vectorizer_enhanced, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(lr_enhanced, \"logistic_regression_model.pkl\")\n",
    "print(\"Enhanced TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'\")\n",
    "print(\"Enhanced Logistic Regression model saved as 'logistic_regression_model.pkl'\")\n",
    "\n",
    "# Save predictions\n",
    "lr_enhanced_output_df = pd.DataFrame({\n",
    "    'True Label': y_test_simple,\n",
    "    'Predicted Label': y_pred_enhanced\n",
    "})\n",
    "lr_enhanced_output_df.to_csv('logistic_regression_predictions.csv', index=False)\n",
    "print(\"Enhanced Logistic Regression predictions saved to 'logistic_regression_predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1acf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Standard Logistic Regression (using preprocessed data)\n",
    "print(\"Training standard logistic regression...\")\n",
    "\n",
    "# Check if we have the required variables from previous cells\n",
    "if 'X_train_tfidf' in locals() and 'y_train' in locals():\n",
    "    lr = LogisticRegression(max_iter=200)\n",
    "    lr.fit(X_train_tfidf, y_train)\n",
    "    y_pred_lr = lr.predict(X_test_tfidf)\n",
    "    \n",
    "    # Evaluate Logistic Regression\n",
    "    show_metrics('Standard Logistic Regression (TF-IDF)', y_test, y_pred_lr)\n",
    "    \n",
    "    # Save the standard model (backup)\n",
    "    joblib.dump(lr, \"logistic_regression_standard.pkl\")\n",
    "    print(\"Standard Logistic Regression model saved as 'logistic_regression_standard.pkl'.\")\n",
    "    \n",
    "    # Save predictions\n",
    "    lr_output_df = pd.DataFrame({\n",
    "        'True Label': y_test,\n",
    "        'Predicted Label': y_pred_lr\n",
    "    })\n",
    "    lr_output_df.to_csv('logistic_regression_standard_predictions.csv', index=False)\n",
    "    print(\"Standard Logistic Regression predictions saved.\")\n",
    "else:\n",
    "    print(\"Warning: Required variables not found. Please run the preprocessing cells first.\")\n",
    "    print(\"Available variables:\", [var for var in locals().keys() if not var.startswith('_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25791c7a",
   "metadata": {},
   "source": [
    "## Train and Evaluate BiLSTM Model\n",
    "\n",
    "We will use a Bidirectional LSTM (BiLSTM) neural network with word embeddings to classify the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a973af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define max_words and max_len if not already defined\n",
    "max_words = 5000\n",
    "max_len = 100\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Build BiLSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train BiLSTM\n",
    "model.fit(X_train_pad, y_train, epochs=2, batch_size=32, verbose=1)\n",
    "y_pred_bilstm = (model.predict(X_test_pad) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Evaluate BiLSTM\n",
    "show_metrics('BiLSTM', y_test, y_pred_bilstm)\n",
    "\n",
    "# Save the BiLSTM model\n",
    "model.save(\"bilstm_model.h5\")\n",
    "print(\"BiLSTM model saved as 'bilstm_model.h5'.\")\n",
    "\n",
    "# Save the tokenizer for BiLSTM\n",
    "import joblib\n",
    "joblib.dump(tokenizer, \"bilstm_tokenizer.pkl\")\n",
    "print(\"BiLSTM tokenizer saved as 'bilstm_tokenizer.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions and true labels to CSV files for all models\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure predictions and true labels are available\n",
    "if 'y_test' in locals():\n",
    "    # Save BiLSTM predictions\n",
    "    if 'y_pred_bilstm' in locals():\n",
    "        bilstm_output_df = pd.DataFrame({\n",
    "            'True Label': y_test,\n",
    "            'Predicted Label': y_pred_bilstm\n",
    "        })\n",
    "        bilstm_output_df.to_csv('bilstm_predictions.csv', index=False)\n",
    "        print(\"BiLSTM predictions saved to 'bilstm_predictions.csv'.\")\n",
    "\n",
    "    # Save Logistic Regression predictions\n",
    "    if 'y_pred_lr' in locals():\n",
    "        lr_output_df = pd.DataFrame({\n",
    "            'True Label': y_test,\n",
    "            'Predicted Label': y_pred_lr\n",
    "        })\n",
    "        lr_output_df.to_csv('logistic_regression_predictions.csv', index=False)\n",
    "        print(\"Logistic Regression predictions saved to 'logistic_regression_predictions.csv'.\")\n",
    "\n",
    "    # Save SVM predictions\n",
    "    if 'y_pred_svm' in locals():\n",
    "        svm_output_df = pd.DataFrame({\n",
    "            'True Label': y_test,\n",
    "            'Predicted Label': y_pred_svm\n",
    "        })\n",
    "        svm_output_df.to_csv('svm_predictions.csv', index=False)\n",
    "        print(\"SVM predictions saved to 'svm_predictions.csv'.\")\n",
    "\n",
    "    # Save BERT predictions\n",
    "    if 'y_pred_bert' in locals():\n",
    "        bert_output_df = pd.DataFrame({\n",
    "            'True Label': y_test,\n",
    "            'Predicted Label': y_pred_bert\n",
    "        })\n",
    "        bert_output_df.to_csv('bert_predictions.csv', index=False)\n",
    "        print(\"BERT predictions saved to 'bert_predictions.csv'.\")\n",
    "else:\n",
    "    print(\"Error: True labels are not available. Train and evaluate the models first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10981473",
   "metadata": {},
   "source": [
    "## Train and Evaluate BERT Model\n",
    "\n",
    "We will use a pre-trained BERT model (via Hugging Face Transformers) to classify the text. This approach uses transformer-based embeddings and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple BERT-like approach using tokenization and neural network\n",
    "from transformers import AutoTokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load BERT tokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode texts using BERT tokenizer\n",
    "def simple_bert_encode(texts, tokenizer, max_len=64):\n",
    "    input_ids = []\n",
    "    for text in tqdm(texts, desc=\"Encoding Texts\"):\n",
    "        encoded = tokenizer.encode(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids.append(encoded)\n",
    "    return np.array(input_ids)\n",
    "\n",
    "# Tokenize training and test data\n",
    "X_train_bert = simple_bert_encode(X_train, bert_tokenizer)\n",
    "X_test_bert = simple_bert_encode(X_test, bert_tokenizer)\n",
    "\n",
    "print(f\"Shape of X_train_bert: {X_train_bert.shape}\")\n",
    "print(f\"Shape of X_test_bert: {X_test_bert.shape}\")\n",
    "\n",
    "# Build a simple neural network model\n",
    "bert_model = Sequential([\n",
    "    Embedding(input_dim=bert_tokenizer.vocab_size, output_dim=128, input_length=64),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "bert_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Simple BERT-based Model...\")\n",
    "bert_model.fit(\n",
    "    X_train_bert, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict using the model\n",
    "y_pred_bert = (bert_model.predict(X_test_bert) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "show_metrics('Simple BERT-based Model', y_test, y_pred_bert)\n",
    "\n",
    "# Save the BERT model\n",
    "bert_model.save(\"bert_model.h5\")\n",
    "print(\"BERT model saved as 'bert_model.h5'.\")\n",
    "\n",
    "# Save the BERT tokenizer\n",
    "import joblib\n",
    "joblib.dump(bert_tokenizer, \"bert_tokenizer.pkl\")\n",
    "print(\"BERT tokenizer saved as 'bert_tokenizer.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f40d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging: Check shapes of tokenized inputs\n",
    "if 'X_train_bert' in locals():\n",
    "    print(\"Shape of X_train_bert:\", X_train_bert.shape)\n",
    "    print(\"Shape of X_test_bert:\", X_test_bert.shape)\n",
    "    print(\"Sample of X_train_bert[0]:\", X_train_bert[0][:10])  # Show first 10 tokens\n",
    "else:\n",
    "    print(\"X_train_bert not defined yet. Run the BERT encoding cell first.\")\n",
    "\n",
    "# Debugging: Check if BERT tokenizer is working\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    print(\"BERT tokenizer loaded successfully.\")\n",
    "    print(\"Vocab size:\", bert_tokenizer.vocab_size)\n",
    "except Exception as e:\n",
    "    print(\"Error loading BERT tokenizer:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM\n",
    "print(\"Training SVM...\")\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate SVM\n",
    "show_metrics('SVM (TF-IDF)', y_test, y_pred_svm)\n",
    "\n",
    "# Save the SVM model\n",
    "import joblib\n",
    "joblib.dump(svm, \"svm_model.pkl\")\n",
    "print(\"SVM model saved as 'svm_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Train SGDClassifier with progress tracking\n",
    "print(\"Training SGDClassifier...\")\n",
    "sgd = SGDClassifier()\n",
    "for epoch in tqdm(range(10), desc=\"SGD Training Progress\"):\n",
    "    sgd.partial_fit(X_train_tfidf, y_train, classes=np.unique(y_train))\n",
    "\n",
    "y_pred_sgd = sgd.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate SGDClassifier\n",
    "show_metrics('SGDClassifier (TF-IDF)', y_test, y_pred_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a492fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-IDF vectorizer for later use\n",
    "import joblib\n",
    "\n",
    "# Ensure the vectorizer is trained before saving\n",
    "if 'vectorizer' in locals():\n",
    "    joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "    print(\"TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'.\")\n",
    "else:\n",
    "    print(\"Error: TF-IDF vectorizer is not defined. Train the vectorizer before saving.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
